{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Module 1: Vanilla GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialization of libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.mixture import GMM\n",
    "device = torch.device('cuda')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameters for the training\n",
    "mb_size = 128 # Batch Size\n",
    "Z_dim = 64  # Length of noise vector\n",
    "X_dim = 1  # Input Length\n",
    "h_dim = 128  # Hidden Dimension\n",
    "lr = 1e-4    # Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "gmm = GMM(5)\n",
    "gmm.means_ = np.array([[10], [20], [60], [80], [110]])\n",
    "gmm.covars_ = np.array([[3], [3], [2], [2], [1]]) ** 2\n",
    "gmm.weights_ = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "X = gmm.sample(200000)\n",
    "data = X\n",
    "data = (data - X.min())/(X.max()-X.min())\n",
    "plt.hist(data, 200000, normed=False, histtype='stepfilled', alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "G = torch.nn.Sequential(\n",
    "    torch.nn.Linear(Z_dim, h_dim),\n",
    "    torch.nn.PReLU(),\n",
    "    torch.nn.Linear(h_dim, h_dim),\n",
    "    torch.nn.PReLU(),\n",
    "    torch.nn.Linear(h_dim, X_dim),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "D = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X_dim, h_dim),\n",
    "    torch.nn.LeakyReLU(0.2),\n",
    "    torch.nn.Linear(h_dim, h_dim),\n",
    "    torch.nn.LeakyReLU(0.2),\n",
    "    torch.nn.Linear(h_dim, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "G = G.cuda()\n",
    "D = D.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we will gather the parameters of the generator and the discriminator so that they can be given to the Adam optimizer to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "G_solver = optim.Adam(G.parameters(), lr)\n",
    "D_solver = optim.Adam(D.parameters(), lr)\n",
    "\n",
    "ones_label = torch.ones(mb_size,1)\n",
    "zeros_label = torch.zeros(mb_size,1)\n",
    "loss = nn.BCELoss()\n",
    "ones_label = ones_label.to(device)\n",
    "zeros_label = zeros_label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the gradients to zero\n",
    "params = [G, D]\n",
    "def reset_grad():\n",
    "    for net in params:\n",
    "        net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Now, we will start the actual training. The training alternates between updating the discriminator network's weights and updating the generator's weight.First, we update the discriminator's weight. We take a minibatch from the dataset and do a forward pass on the discriminator with the label '1'. Then, we feed noise into the generator and feed the generated images into the discriminator with the label '0'. We backpropagate the error and update the discriminator weights. To update the generator weights, we feed noise to the generator and feed the generated images into the discriminator with the label '1'. This error is backpropagated to update the weights of G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "for it in range(198000):\n",
    "    \n",
    "    \n",
    "    # ###\n",
    "    if ((data_index + 1)*mb_size>len(data)):\n",
    "        data_index = 0\n",
    "    # ###\n",
    "    #z = torch.randn(mb_size, Z_dim)\n",
    "    z = torch.FloatTensor(mb_size, Z_dim).uniform_(-1, 1)\n",
    "    X = torch.from_numpy(np.array(data[data_index*mb_size : (data_index + 1)*mb_size]))\n",
    "    X = X.view(mb_size, 1)\n",
    "    X = X.type(torch.FloatTensor)\n",
    "    X = X.to(device)\n",
    "    z = z.to(device)\n",
    "    \n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    #forward pass\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    D_loss_real = loss(D_real, ones_label)\n",
    "    D_loss_fake = loss(D_fake, zeros_label)\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    # Calulate and update gradients of discriminator\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    \n",
    "    #z = torch.randn(mb_size, Z_dim)\n",
    "    z = torch.FloatTensor(mb_size, Z_dim).uniform_(-1, 1)\n",
    "    z = z.to(device)\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = loss(D_fake, ones_label)\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # reset gradient\n",
    "    reset_grad()\n",
    "    data_index = data_index + 1\n",
    "    # Print and plot every now and then\n",
    "    if it % 10000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.cpu().numpy(), G_loss.data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "final = np.zeros(1500*mb_size, dtype = float)\n",
    "for i in range(1500):\n",
    "    z = torch.FloatTensor(64, Z_dim).uniform_(-1, 1)\n",
    "    z = z.to(device)\n",
    "    l = G(z).cpu().detach().numpy()\n",
    "    final[i*mb_size : ((i+ 1)*mb_size -1)] = l[0]\n",
    "p1 = plt.hist(final, 200, normed=True, histtype='bar', alpha=0.5)\n",
    "p2 = plt.hist(data, 200, normed=True, histtype='bar', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
